
SUPERVISED LEARNING				Input Data & Output Data		Remember & Generalize

	Regression			If the value to predict is Continuous

		Linear Regression			Linear Spread of data
		Polynomial Regression		non-Linear Spread of data
		DecisionTree Regressor		non-Linear Spread of data
		RandomForest Regressor		non-Linear Spread of data

	Classification		If the value to predict is Discrete
		
		Logistic Regression
		K-Nearest Neighbours
		Support Vector Machine
		Naive Bayes
		DecisionTree Classifier
		RandomForest Classifier	



Data preprocessing

Encoding / Dummy variable creation / OneHotEncoding

numOfDummy = numOfUniqueVal-1

California		d0_C							
Florida		d1_F							
New York		d2_NY						


State			d0_C		d1_F		d2_NY
	
New York		0		0		1
California		1		0		0
Florida		0		1		0
New York		0		0		1
Florida		0		1		0
New York		0		0		1
California
Florida
New York
California
Florida
California




Polynomial Regression

	degree			Hyper-Parameter

	
	Hyper-Parameter	parameters whise value we find with Trial & Error

	Hyper-Parameter Tunning


Broad Search	

	degree		2	5	10	15	20

Guided Search

	degree		5	6	7	8	9	10


y_pred = b0 + b1*power(x1,1) + b2*power(x1,2) + b3*power(x1,3) + b4*power(x1,4) + b5*power(x1,5)
 



DecisionTree Regressor		non-Linear Spread of data


Step1: Find the best split for Root Node

Step2: Should we Further Split to Construct a branch or make it a leaf node ??

		if the Split contains values equal to or more than "n" data points in it
		Then
			SPLIT FURTHER
		Else
			MAKE IT LEAF NODE

		Value for n = ??? is set by developer , default n= 2		(n is Hyper-parameter)	


		for our example value of n  = 7

		we only split further if we have 7 or more data points in that split



Underfitting		Model Not learned Enough		The Error on Test Data will be very High
										The Error will High accross multiple batches of test data	 

Overfitting			Model Learned Too Much		The Error on Training Data will be low
										The Error on Test Data will be High
										The Error will not be stable accross multiple batches of test data	 

Good Fit			Model Learned well / enough	The Error on Test Data will be Low
										The Error will be stable accross multiple batches of test data	 




RandomForest Regressor	


RandomForest

NumOfTrees = To be decided by developers (Trial & Error)

RandomForest Tree Creation		(X_train & y_train)		38 samples

Step1 	=> BootStrap Dataset creation i.e. Random Row selection from the given training set
		=> Bootstrap dataset will be SAME SIZE as Training Dataset
				e.g. X_train, y_train 		38 samples
					Bootstrap dataset		38 samples

		=> But Bootstrap dataset does not use all 38 values in the training set to create the Bootstrap dataset
				it just uses 2/3 of the total values in the training set to create the Bootstrap dataset

				2/3(training set)	--------> 66% rows , Bagging , remainder is called out of Bag Records
				2/3 (38 rows)		---------> 25 rows random pick of values from training set (66%)

		=> Bootstrap dataset after taking 2/3 unique rows the remainder rows are filledup to match the training set size by 						duplicating rows from the 2/3 volume






RandomForest (10 trees)

Tree1	Bootstrap Dataset1
Tree2	Bootstrap Dataset2
Tree3	Bootstrap Dataset3
..........
Tree10	Bootstrap Dataset10




Training Sample
Row1	pattern1
Row2	pattern2
Row3	pattern1
Row4	pattern1
Row5	pattern2
Row6	pattern1
Row7	pattern3
Row8	pattern2
Row9	pattern2
Row10	pattern1
Row11	pattern2


Bootstrap Dataset1
Row1	pattern1
Row2	pattern2
Row3	pattern1
Row4	pattern1
Row5	pattern2
Row6	pattern1
Row7	pattern3
Row6	pattern1
Row7	pattern3
Row7	pattern3
Row5	pattern2





Step2 => Random Choice of X features for Root Node / Branch Creation

	Total 5 features 				max_features =AUTO


Tree1		Bootstrap dataset1

				Root			X1	X5
				Branch1		X4	X3
				Branch2		X2	X1




Ensemble Learning

		Hybrid Models

		multiple models put together and return the final output

		RF

			DT model1		DT model2		DT model3	........... avg(all)




	UseCase	

Polynomial Regression		Error		12.46			
DecisionTree Regressor		Error		15.78
RandomForest Regressor 		Error		0.84			least error


Polynomial Regression		Error		0.46			simple model
DecisionTree Regressor		Error		0.44
RandomForest Regressor 		Error		0.45


Polynomial Regression		Error		120.46
DecisionTree Regressor		Error		39.90		Lighter / Simple Model		
RandomForest Regressor 		Error		38.45		numOfTrees = 300







		Age		Salary	BankBalance	CreditSCore	OwnHouse

		24		25000	75000		650			1





	Classification		If the value to predict is Discrete
		
		Logistic Regression
		K-Nearest Neighbours			93%		slow performance, during predict does all the learning
		Support Vector Machine			93%
		Naive Bayes					90%
		DecisionTree Classifier			94%			max_depth=2
		RandomForest Classifier			94%			trees=10, max_Depth=2







Baye's Theorem

Probability of some event					P(A|B)
Given some new information 				P(B|A)
and a Priori belief in the probability of event ,	 P(A)

P(A|B) = P(B|A) * P(A)   
		--------------
		     P(B)

P(y | x1,x2,x3......xn)  = P(x1,x2,x3.......xn | y) * P(y)
					------------------------------
						P(x1,x2,x3.......xn)	


P(y | x1,x2,x3......xn)		Posterior probability

P(x1,x2,x3.......xn | y)		Likelihood

P(y)						Prior 

P(x1,x2,x3.......xn)			marginal probability




















